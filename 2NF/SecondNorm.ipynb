{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Loc_Pt\n",
    "# Load your data \n",
    "df = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_Loc_Pt.csv')\n",
    "\n",
    "# Dynamically select columns (e.g., all columns except a specific one)\n",
    "columns_to_keep = [col for col in df.columns if col not in ['Ftr_ID *', 'Ftr_Name', 'Other_Name']]\n",
    "\n",
    "# 1. Create a table for point (removing partial dependency)\n",
    "point = df[columns_to_keep].drop_duplicates()\n",
    "\n",
    "# 2. Create a table for feature\n",
    "feature1 = df[['Ftr_ID *', 'Ftr_Name', 'Other_Name']].drop_duplicates()\n",
    "\n",
    "# 3. Save the resulting tables to CSV files or databases \n",
    "point.to_csv('siteLocPt.csv', index=False)\n",
    "feature1.to_csv('featureLocPt.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create shape table\n",
    "shape1 = df[['Shape *']].drop_duplicates().reset_index(drop=True)\n",
    "shape1.to_csv('shapePt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create ref table\n",
    "ref1 = df[['Ref_ID']].drop_duplicates().reset_index(drop=True)\n",
    "ref1.to_csv('refPt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/kq0_38s55sd9wy3rc90p9t240000gn/T/ipykernel_14937/3955235259.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comm_unique['Com_ID'] = 'CM_' + (comm_unique.reset_index().index + 1).astype(str).str.zfill(2)\n"
     ]
    }
   ],
   "source": [
    "## create commodity table\n",
    "df = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_Site.csv')\n",
    "commodity = pd.DataFrame(df)\n",
    "\n",
    "# Strip leading and trailing spaces from the Commodity column\n",
    "commodity['Commodity'] = commodity['Commodity'].str.strip()\n",
    "\n",
    "# Remove duplicate values from the Commodity column\n",
    "comm_unique = commodity.drop_duplicates(subset='Commodity')\n",
    "\n",
    "# Add a new column 'Com_ID' with the desired format SC_0 followed by an ascending counting number\n",
    "comm_unique['Com_ID'] = 'CM_' + (comm_unique.reset_index().index + 1).astype(str).str.zfill(2)\n",
    "\n",
    "# Reorder columns to place 'Com_ID' before 'Commodity'\n",
    "comm_unique = comm_unique[['Com_ID', 'Commodity']]\n",
    "\n",
    "# Save to CSV (without index column)\n",
    "comm_unique.to_csv('commodityLocPt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loc_Poly\n",
    "# Load your data\n",
    "df1 = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/Loc_Poly.csv')\n",
    "\n",
    "# Dynamically select columns (e.g., all columns except a specific one)\n",
    "columns_to_keep = [col for col in df1.columns if col not in ['Shape *', 'Ftr_ID', 'Ftr_Name']]\n",
    "\n",
    "# 1. Create a table for site (removing partial dependency)\n",
    "sitee = df1[columns_to_keep].drop_duplicates()\n",
    "\n",
    "# 2. Create a table for feature\n",
    "feature2 = df1[['Ftr_ID', 'Ftr_Name']].drop_duplicates()\n",
    "\n",
    "# 3. Save the resulting tables to CSV files or databases \n",
    "sitee.to_csv('sitePoly.csv', index=False)\n",
    "feature.to_csv('featureLocPoly.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create shape table\n",
    "shape2 = df1[['Shape *']].drop_duplicates().reset_index(drop=True)\n",
    "shape2.to_csv('shapePoly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the relations table \n",
    "dfpo = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/Loc_Poly.csv')\n",
    "# Specify the columns to include in the relation table\n",
    "columns_to_include = ['Shape *', 'Site_ID', 'Ftr_ID']  \n",
    "\n",
    "# Step 1: Create a new DataFrame with the specified columns\n",
    "relation_Loc_Poly = dfpo[columns_to_include].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Step 2: create a new column 'SiteFtCom_ID' with the specified format\n",
    "relation_Loc_Poly['POLY_ID'] = (\n",
    "    'P_0' + (relation_Loc_Poly.index + 1).astype(str)\n",
    ")\n",
    "\n",
    "# Step 3: Move this column to the first position\n",
    "columns = ['POLY_ID'] + [col for col in relation_Loc_Poly.columns if col != 'SiteFt_ID']\n",
    "relation_Loc_Poly = relation_Loc_Poly[columns]\n",
    "\n",
    "relation_Loc_Poly.to_csv('relation_Loc_Poly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loc_Poly_Sw\n",
    "dfsw = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/Loc_Poly_Sw.csv')\n",
    "## create shape table\n",
    "shapesw = dfsw[['Shape *']].drop_duplicates().reset_index(drop=True)\n",
    "shapesw.to_csv('shapeSw.csv', index=False)\n",
    "## create ref table\n",
    "refsw = dfsw[['Ref_ID']].drop_duplicates().reset_index(drop=True)\n",
    "refsw.to_csv('refSw.csv', index=False)\n",
    "## Creating the relations table \n",
    "columns_to_include = ['Shape *', 'Ftr_ID', 'Ref_ID']  \n",
    "\n",
    "# Step 1: Create a new DataFrame with the specified columns\n",
    "relation_sw = dfsw[columns_to_include].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Step 2: create a new column 'SiteFtCom_ID' with the specified format\n",
    "relation_sw['SW_ID'] = (\n",
    "    'SW_0' + (relation_sw.index + 1).astype(str)\n",
    ")\n",
    "\n",
    "# Step 3: Move this column to the first position\n",
    "columns = ['SW_ID'] + [col for col in relation_sw.columns if col != 'SiteFt_ID']\n",
    "relation_sw = relation_sw[columns]\n",
    "\n",
    "relation_sw.to_csv('relationSw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/kq0_38s55sd9wy3rc90p9t240000gn/T/ipykernel_14937/900673629.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfsite_unique['Com_ID'] = 'CM_' + (dfsite_unique.reset_index().index + 1).astype(str).str.zfill(2)\n"
     ]
    }
   ],
   "source": [
    "### Site\n",
    "# Load data\n",
    "siteDt = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_Site.csv')\n",
    "dfsite = pd.DataFrame(siteDt)\n",
    "\n",
    "# Step 1: Strip leading and trailing spaces from the Commodity column\n",
    "dfsite['Commodity'] = dfsite['Commodity'].str.strip()\n",
    "\n",
    "# Step 2: Remove duplicate values from the Commodity column\n",
    "dfsite_unique = dfsite.drop_duplicates(subset='Commodity')\n",
    "\n",
    "# Step 3: Add a new column 'SC_ID' with the desired format SC_0 followed by an ascending counting number\n",
    "dfsite_unique['Com_ID'] = 'CM_' + (dfsite_unique.reset_index().index + 1).astype(str).str.zfill(2)\n",
    "\n",
    "# Reorder columns to place 'SC_ID' before 'Commodity'\n",
    "dfsite_unique = dfsite_unique[['Com_ID', 'Commodity']]\n",
    "\n",
    "# Step 4: Save to CSV (without index column)\n",
    "dfsite_unique.to_csv('commoditySite.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siteDt = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_Site.csv')\n",
    "\n",
    "# Dynamically select columns (e.g., all columns except a specific one)\n",
    "columns_to_keep = [col for col in siteDt.columns if col not in ['Commodity']]\n",
    "\n",
    "# 1. Create a table for site (removing partial dependency)\n",
    "siteSite = siteDt[columns_to_keep].drop_duplicates()\n",
    "\n",
    "# 2. Save the resulting tables to CSV files or databases \n",
    "siteSite.to_csv('siteSite.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue for Site\n",
    "# Creating the relations table \n",
    "dfsite = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_Site.csv')\n",
    "\n",
    "# Specify the columns to include in the relation table\n",
    "columns_to_include = ['Site_ID *', 'Commodity']  \n",
    "\n",
    "# Step 1: Create a new DataFrame with the specified columns\n",
    "relation_Site = dfsite[columns_to_include].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Step 2: create a new column 'SiteCom_ID' with the specified format\n",
    "relation_Site['Site_ID'] = (\n",
    "    'S_0' + (relation_Site.index + 1).astype(str)\n",
    ")\n",
    "\n",
    "# Step 3: Move this column to the first position\n",
    "columns = ['Site_ID'] + [col for col in relation_Site.columns if col != 'Site_ID']\n",
    "relation_Site = relation_Site[columns]\n",
    "\n",
    "# Step 4: Save or display the relation table\n",
    "relation_Site.to_csv('relation_Site.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GeolMinOcc\n",
    "# Load your data \n",
    "df2 = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_GeolMinOcc.csv')\n",
    "\n",
    "## Create commodity table\n",
    "# Strip leading and trailing spaces from the Commodity column\n",
    "df2['Commodity'] = df2['Commodity'].str.strip()\n",
    "\n",
    "# Remove duplicate values from the Commodity column\n",
    "commm_unique = df2.drop_duplicates(subset='Commodity')\n",
    "\n",
    "# Add a new column 'Com_ID' with the desired format SC_0 followed by an ascending counting number\n",
    "#commm_unique['Com_ID'] = 'CM_' + (commm_unique.reset_index().index + 1).astype(str).str.zfill(2)\n",
    "\n",
    "# Reorder columns to place 'Com_ID' before 'Commodity'\n",
    "commm_unique = comm_unique[['Commodity']]\n",
    "\n",
    "# Save to CSV (without index column)\n",
    "commm_unique.to_csv('commodityGeol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create feature table\n",
    "ftGeo = df2[['Ftr_ID *', 'Ftr_Name']].drop_duplicates()\n",
    "ftGeo.to_csv('featureGeol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create minType table\n",
    "# Normalize by extracting unique rows based on the 'Type' column\n",
    "ming = df2[['Ftr_Type']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Create a 'Type_ID' column with the syntax 'T_0' followed by counting number\n",
    "ming['Type_ID'] = 'T_' + (ming.index + 1).astype(str).str.zfill(2)\n",
    "\n",
    "# Reorder columns to place 'Type_ID' as the first column\n",
    "ming = ming[['Type_ID', 'Ftr_Type']]\n",
    "ming.to_csv('minTypeGeol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create ref table\n",
    "refGeol = df2[['Ref_ID']].drop_duplicates().reset_index(drop=True)\n",
    "refGeol.to_csv('refGeol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create site table\n",
    "# Dynamically select columns (e.g., all columns except a specific one)\n",
    "columns_to_keep = [col for col in df2.columns if col not in ['OID *', 'Ftr_ID *', 'Ftr_Name', 'Ftr_Type', 'Commodity', 'Ref_ID']]\n",
    "\n",
    "# Create a table for point (removing partial dependency)\n",
    "sg = df2[columns_to_keep].drop_duplicates()\n",
    "\n",
    "# Save the resulting tables to CSV files or databases \n",
    "sg.to_csv('siteGeol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create RELATIONS within GeolMinOcc\n",
    "df2 = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/1NF/1NF_GeolMinOcc.csv')\n",
    "\n",
    "columns_to_include = ['Site_ID *', 'Ftr_ID *', 'Ftr_Type', 'Commodity', \"Ref_ID\"]  \n",
    "\n",
    "# Step 1: Create a new DataFrame with the specified columns\n",
    "relationGeol = df2[columns_to_include].reset_index(drop=True)\n",
    "\n",
    "# Step 2: create a new column '_ID' with the specified format\n",
    "relationGeol['GEO_ID'] = (\n",
    "    'G_0' + (relationGeol.index + 1).astype(str)\n",
    ")\n",
    "\n",
    "# Step 3: Move this column to the first position\n",
    "columns = ['GEO_ID'] + [col for col in relationGeol.columns if col != 'GEO_ID']\n",
    "relationGeol = relationGeol[columns]\n",
    "relationGeol.to_csv('relationGeol.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "### History\n",
    "# Load your data \n",
    "df3 = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/History.csv')\n",
    "\n",
    "##Create a table for site\n",
    "site = df3[['Site_ID *', 'Last_Updt']].drop_duplicates()\n",
    "site.to_csv('siteHist.csv', index=False)\n",
    "\n",
    "##Create a table for feature\n",
    "ftr = df3[['Ftr_ID *', 'Ftr_Name']].drop_duplicates()\n",
    "ftr.to_csv('featureHist.csv', index=False)\n",
    "##Create a table for reference\n",
    "refH = df3[['Ref_ID']].drop_duplicates()\n",
    "refH.to_csv('refHist.csv', index=False)\n",
    "##Create a table for status detail\n",
    "stt = df3[['StatDetail', 'Year_From', 'Year_To', 'Ref_Detail', 'Remarks']].drop_duplicates()\n",
    "\n",
    "##Create a table for status \n",
    "status = df3[['Status']].drop_duplicates()\n",
    "status['Status_ID'] = (\n",
    "    'S_0' + (status.index + 1).astype(str).str.zfill(2)\n",
    ")\n",
    "columns = ['Status_ID'] + [col for col in status.columns if col != 'Status_ID']\n",
    "status = status[columns]\n",
    "status.to_csv('status.csv', index=False)\n",
    "stt.to_csv('statusDetail.csv', index=False)\n",
    "\n",
    "#create relation table\n",
    "columns_to_include = ['Site_ID *', 'Ftr_ID *', 'Status', 'StatDetail', \"Ref_ID\"]  \n",
    "\n",
    "# Step 1: Create a new DataFrame with the specified columns\n",
    "relationHist = df3[columns_to_include].reset_index(drop=True)\n",
    "\n",
    "# Step 2: create a new column '_ID' with the specified format\n",
    "relationHist['History_ID'] = (\n",
    "    'H_0' + (relationHist.index + 1).astype(str)\n",
    ")\n",
    "\n",
    "# Step 3: Move this column to the first position\n",
    "columns = ['History_ID'] + [col for col in relationHist.columns if col != 'History_ID']\n",
    "relationHist = relationHist[columns]\n",
    "relationHist.to_csv('relationHist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OBJECTID * Site_ID *  Last_Updt                      DpMd_NoNm Ref_Detail  \\\n",
      "0            1   ak00012  6/12/2018         32c - kipushi cu-pb-zn   page 227   \n",
      "1            2   ak00012  6/12/2018         32c - kipushi cu-pb-zn   page 227   \n",
      "2            3   ak00205  4/16/2018  synorogenic-synvolcanic ni-cu     <null>   \n",
      "3            4   ak00205  4/16/2018  synorogenic-synvolcanic ni-cu     <null>   \n",
      "4            5   ak00205  4/16/2018  synorogenic-synvolcanic ni-cu     <null>   \n",
      "..         ...       ...        ...                            ...        ...   \n",
      "57          58   pr00004  7/17/2018        nickel-cobalt laterites    page 10   \n",
      "58          59   pr00004  7/17/2018        nickel-cobalt laterites    page 10   \n",
      "59          60   pr00005  7/17/2018        nickel-cobalt laterites    page 10   \n",
      "60          61   pr00005  7/17/2018        nickel-cobalt laterites    page 10   \n",
      "61          62   tn00001  5/11/2018                massive sulfide    page 13   \n",
      "\n",
      "                                           DpMd_RefID           GEM_Name  \\\n",
      "0                            cox and bernstein (1986)       unclassified   \n",
      "1                            cox and bernstein (1986)       unclassified   \n",
      "2                                reed and dorr (1942)  magmatic sulfides   \n",
      "3                                reed and dorr (1942)  magmatic sulfides   \n",
      "4                                reed and dorr (1942)  magmatic sulfides   \n",
      "..                                                ...                ...   \n",
      "57                            marsh and others (2013)             <null>   \n",
      "58                            marsh and others (2013)             <null>   \n",
      "59                            marsh and others (2013)             <null>   \n",
      "60                            marsh and others (2013)             <null>   \n",
      "61  u.s. geological survey and u.s. bureau of mine...             <null>   \n",
      "\n",
      "         GEM_RefID Remarks  \n",
      "0           <null>  <null>  \n",
      "1           <null>  <null>  \n",
      "2   du bray (1995)  <null>  \n",
      "3   du bray (1995)  <null>  \n",
      "4   du bray (1995)  <null>  \n",
      "..             ...     ...  \n",
      "57          <null>  <null>  \n",
      "58          <null>  <null>  \n",
      "59          <null>  <null>  \n",
      "60          <null>  <null>  \n",
      "61          <null>  <null>  \n",
      "\n",
      "[62 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "### Dep_Model\n",
    "# Load your data \n",
    "df4 = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/Dep_Model.csv')\n",
    "\n",
    "# Dynamically select columns (e.g., all columns except a specific one)\n",
    "columns_to_keep = [col for col in df4.columns if col not in ['Ftr_Name', 'Ftr_ID *']]\n",
    "\n",
    "# Strip whitespaces and convert to lowercase for all string columns\n",
    "df4 = df4.apply(lambda x: x.str.strip().str.lower() if x.dtype == \"object\" else x)\n",
    "site1 = df4.drop_duplicates()\n",
    "\n",
    "# 1. Create a table for status (removing partial dependency)\n",
    "site1 = df4[columns_to_keep].drop_duplicates()\n",
    "\n",
    "# 2. Create a table for site\n",
    "feature4 = df4[['Ftr_ID *', 'Ftr_Name']].drop_duplicates()\n",
    "\n",
    "# 3. Save the resulting tables to CSV files or databases \n",
    "site1.to_csv('1stExamSiteDepsite.csv', index=False)\n",
    "feature4.to_csv('featureDep.csv', index=False)\n",
    "print(site1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like there are still redundancy after this first examination. continue to split this Site table \n",
    "# into Site and Deposit table \n",
    "\n",
    "#create new site table\n",
    "newdf = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/2NF/Dep_Model/1stExamSiteDep.csv', encoding='utf-8')\n",
    "newsite = newdf[['Site_ID *', 'Last_Updt']].drop_duplicates()\n",
    "newsite.to_csv('finalSiteDep.csv', index=False)\n",
    "\n",
    "# create dep table\n",
    "dep = newdf[['DpMd_NoNm', 'Ref_Detail']].drop_duplicates()\n",
    "#dep['Site_ID *'] = dep['Site_ID *'].str.upper()\n",
    "dep.to_csv('dep.csv', index=False)\n",
    "# Create a 'Dep_ID' column with the syntax 'D_0' followed by counting number\n",
    "#dep['Dep_ID'] = 'D_' + (dep.index + 1).astype(str).str.zfill(2)\n",
    "#dep = dep[['Dep_ID'] + columns_to_keep].drop_duplicates()\n",
    "#dep.to_csv('dep.csv', index=False)\n",
    "#create depRef table\n",
    "\n",
    "depre = newdf[['DpMd_RefID']].drop_duplicates()\n",
    "depre.to_csv('depRef.csv', index=False)\n",
    "\n",
    "#create gem table\n",
    "gem = newdf[['GEM_Name']].drop_duplicates()\n",
    "gem.to_csv('gem.csv', index=False)\n",
    "\n",
    "#create gemRef table\n",
    "gemre = newdf[['GEM_RefID']].drop_duplicates()\n",
    "dep.to_csv('gemRef.csv', index=False)\n",
    "\n",
    "df4 = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/Dep_Model.csv', encoding='utf-8')\n",
    "columns_to_include = ['Site_ID *', 'Ftr_ID *', 'DpMd_NoNm', 'DpMd_RefID', 'GEM_Name', 'GEM_RefID']\n",
    "\n",
    "# Step 1: Create a new DataFrame with the specified columns\n",
    "relationDep = df4[columns_to_include].reset_index(drop=True)\n",
    "\n",
    "# Step 2: create a new column '_ID' with the specified format\n",
    "relationDep['DEP_ID'] = (\n",
    "    'D_0' + (relationDep.index + 1).astype(str)\n",
    ")\n",
    "\n",
    "# Step 3: Move this column to the first position\n",
    "columns = ['DEP_ID'] + [col for col in relationDep.columns if col != 'DEP_ID']\n",
    "relationDep = relationDep[columns]\n",
    "relationDep.to_csv('relationDep.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Descr_Sum\n",
    "dfdes = pd.read_csv('/Users/nguyenngoc/Documents/CSE385 /Final Project/USGS_Cobalt_US_CSV/Descr_Sum.csv', encoding='latin1')\n",
    "\n",
    "## create ref table\n",
    "refde = dfdes[['Ref_ID']].drop_duplicates().reset_index(drop=True)\n",
    "refde.to_csv('refDes.csv', index=False)\n",
    "## create Des table\n",
    "columns_to_keep = [col for col in dfdes.columns if col not in ['OBJECT_ID *','Ref_ID']]\n",
    "des = dfdes[columns_to_keep]\n",
    "des.to_csv('des.csv', index=False)\n",
    "##relation table\n",
    "columns_to_include = ['Site_ID *', 'Ftr_ID *', 'Descr', 'Ref_ID']  \n",
    "\n",
    "relationdes = dfdes[columns_to_include].reset_index(drop=True)\n",
    "# Add the DES_ID column\n",
    "relationdes['DES_ID'] = (\n",
    "    'DS_0' + (relationdes.index + 1).astype(str).str.zfill(2)  # Add leading zeros for consistency\n",
    ")\n",
    "# Move the 'SiteStatus_ID' column to the first position\n",
    "columns = ['DES_ID'] + [col for col in relationdes.columns if col != 'DES_ID']\n",
    "relationdes = relationdes[columns]\n",
    "relationdes.to_csv('relationDes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
